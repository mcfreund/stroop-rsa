%% about ----
%% mike freund, 2019-02-28
%% the 'third' group analysis of stroop behavioral data.

\documentclass{article}
\usepackage[margin = 0.5in]{geometry}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{amsmath,amssymb}
\usepackage[natbibapa]{apacite}
\bibliographystyle{apacite}

\renewcommand{\familydefault}{\sfdefault}  %% sans-serif
\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}
\DeclareMathOperator{\EX}{\mathbb{E}}  % expected value

\newcommand*{\mono}{\fontfamily{qcr}\selectfont}
\begin{document}

\title{response time and accuracy analyses of Stroop data, DMCC2}
\author{mike freund}
\date{\today}
\maketitle



<<setup, echo = FALSE, warning = FALSE, message = FALSE, error = FALSE, results = FALSE>>=

## packages ----
library(knitr)
knitr::opts_chunk$set(
  echo = FALSE, warning = FALSE, message = FALSE, error = FALSE, results = FALSE, cache = TRUE,
  fig.align = "center", dev = "pdf"
)
library(dplyr)
library(data.table)
library(purrr)
library(ggplot2)
theme_set(theme_minimal(base_size = 10))
library(grid)
library(gridExtra)
library(colorspace)
library(here)
library(lme4)
source(here("..", "gen", "funs", "_get_dirs_local.R"))
source(here("..", "gen", "funs", "_funs.R"))
source(here("r", "group-201902", "_get_misc_vars.R"))


## read sheets ----

group201902.analysis <- read.csv(
  file.path(dir.box.stroop.sheets, "subj-sample_group-201902_for-analysis.csv"),
  stringsAsFactors = FALSE
  )[, 1]
stroop <- read.csv(
  "C:/Users/mcf/Box/global/wustl/proj/cross-task/sheets/dmcc2_behavior-and-events-stroop.csv",
  stringsAsFactors = FALSE
  ) %>%
  arrange(subj, session, run, trial.num) %>%
  group_by(subj, session, run) %>%
  mutate(
    error         = 1 - acc,
    post.error    = lag(error),
    n1.trial.type = lag(trial.type),
    n2.trial.type = lag(n1.trial.type)
    )
subj.summary <- read.csv(
  "C:/Users/mcf/Box/global/wustl/proj/cross-task/sheets/dmcc2_all-runs-summary.csv",
  stringsAsFactors = FALSE
  ) %>% filter(task == "str", subj %in% unique(stroop$subj))

## vars ----

color.congruency <- c("i" = "#b2182b", "c" = "#2166ac")
color.session <- c(bas = "#1b9e77", pro = "#d95f02", rea = "#7570b3")
sessions <- c("bas", "pro", "rea")

@
  
\section{outline}

I have three goals for these analyses.
\begin{enumerate}
  \item determine whether we should (not) trust these RT and accurcacy data (i.e., to reflect cognitive processes)
    \begin{itemize}
      \item via testing for other, well-established effects: post-error slowing, congruency sequence effect, LWPC
    \end{itemize}
  \item build good models of behavioral data that will furnish estimates of each subject's stroop effect
    \begin{itemize}
      \item via parsimonious model building (start simple), and model diagnostics
    \end{itemize}
  \item assess whether there is adequate variance among subject's stroop effects to model with brain measures
    \begin{itemize}
      \item via model comparisons
    \end{itemize}
\end{itemize}

\section{subject sample}

<<>>=
## get twinpairs to remove from new sample"
twinpairs <- subj.summary %>% filter(subj %in% group201902.analysis, !is.na(twin.pair)) %>% .$twin.pair %>% unique
cotwins.to.rm <- subj.summary %>% filter(!subj %in% group201902.analysis, twin.pair %in% twinpairs) %>% .$subj %>% unique
samp1 <- subj.summary %>% filter(!subj %in% cotwins.to.rm)  ## revised sample
samp1 %>% filter(!is.na(twin.pair),  !twin.pair %in% twinpairs)  ## check for twins still remaining
## cotwin pairs  9 and 27 are still both in sample.
## (pair 1 isn't after removing the subject with missing fewer runs.)
set.seed(32)
holdout.these <- c(
  sample(c("162026", "568963"), 1, prob = c(0.5, 0.5)),
  sample(c("198855", "623844"), 1, prob = c(0.5, 0.5)),
  "DMCC3204738"
  )
samp.behavior <- bind_rows(
  samp1 %>% filter(!subj %in% holdout.these),
  subj.summary %>% filter(is.na(twin.pair))
)

length(unique(samp.behavior$subj))  ## num total subjects
## sanity check:
samp.behavior %>%
  filter(session == "pro", run == "1", !is.na(twin.pair)) %>%
  .$twin.pair %>%
  duplicated %>% any
samp.behavior$is.in.group201902 <- samp.behavior$subj %in% group201902.analysis
@


Printed below is the subject summary list.
Note that there are more subjects in this list ($N = \Sexpr{length(unique(samp.behavior$subj))}$) than in the RSA group (group201902\textunderscore analysis, $N = \Sexpr{length(group201902.analysis)}$).
These subjects are all unrelated.

<<results = TRUE>>=
samp.behavior %>% select(subj, session, run, per.have.rt.stroop, missing.rows)
stroop <- stroop %>% filter(subj %in% unique(samp.behavior$subj))
stroop.rt <- stroop %>% filter(acc == 1, !is.na(rt), rt > 0)
stroop.er <- stroop %>% filter(!is.na(acc))
@


\section{do we trust these data?}

\subsection{response times}

\subsubsection{the data}

range: \Sexpr{range(stroop.rt$rt)}

<<rt-session-hists, fig.width = 6, fig.height = 2>>=
stroop.rt %>%
  ggplot(aes(rt)) +
  geom_histogram(aes(fill = session), size = 0.1, binwidth = 20) +
  facet_wrap(~ session) +
  scale_fill_manual(values = color.session) +
  labs(title = "all correct trials") +
  theme(legend.position = "none")
@


<<rt-session-qqs, fig.width = 6, fig.height = 2>>=
stroop.rt %>%
  ggplot(aes(sample = rt, color = session)) +
  stat_qq(alpha = 0.3, size = 0.15) +
  stat_qq_line(size = 0.25) +
  facet_wrap(~ session) +
  scale_color_manual(values = color.session) +
  theme(legend.position = "none")
@

<<rt-allsubjs, fig.height = 4>>=
for (session.i in sessions) {
  p <- stroop.rt %>%
    filter(session == session.i) %>%
    ggplot(aes(pc, rt)) +
    geom_point(
      aes(color = trial.type), position = position_jitterdodge(),
      size = 0.1, alpha = 0.3
      ) +
    facet_wrap(~ subj) +
    theme_minimal(base_size = 5) +
    theme(axis.title = element_blank(), legend.position = "none") +
    scale_color_manual(values = color.congruency) +
    labs(title = session.i)
  print(p)
}
@

<<rt-qqs, fig.height = 5>>=
qqr2 <- function(vec, fun = qnorm, ...) {
  vec <- sort(vec)
  q <- fun(p = seq_along(vec) / (length(vec) + 1), ...)
  cor(q, vec)^2
}


for (session.i in sessions) {
  p <- stroop.rt %>%
    filter(session == session.i) %>%
    full_join(group_by(., subj) %>% summarize(r2norm = qqr2(rt)), by = "subj") %>%
    ggplot(aes(sample = rt)) +
    stat_qq(alpha = 0.3, size = 0.15) +
    stat_qq_line(size = 0.25) +
    facet_wrap(~subj) +
    geom_text(
      aes(
        x = -1.25, y = 2000, 
        label = paste0("r^2 = ", round(r2norm, 3))
        ), size = 1.75, color = "grey50"
    ) +
    theme_minimal(base_size = 5) +
    theme(axis.title = element_blank()) +
    labs(
      title    = paste0("QQ rt versus normal ", session.i),
      subtitle = paste0("overall r^2 with normal = ", round(qqr2(stroop.rt$rt), 3))
        )
    print(p)
}

@

\paragraph{thoughts.}
Nothing super concerning concerning in subject RTs to my eyes, but maybe a little odd that some subjects have very tight distributions.
Might want to set a lower-bound threshold for reseponses (e.g., 200?)---something pretty liberal may work; there don't seem to be too many RTs earlier than 400--500 ms.
Also, there seems to be considerable variance across subjects (and in certain sessions for certain subjects).
This might in part be related to microphone differences.
Will consider modeling each subject with a unique variance term.
(But should note that there may be differences in microphones across sessions within the same subject).

PS: on second look, two subjects do seem to have 500-ms clustering, like the `click' artifact. 
Subjs 161832 and 849971, seen as the kink in their QQ plots.
I'll threshold these subjects' data at rt $> 500$ to eliminate this issue.
Below are their cleaned RT distributions.

<<>>=
stroop.rt <- stroop.rt[!(stroop.rt$subj %in% c("849971", "161832") & stroop.rt$rt < 500), ]

@

<<fig.height = 1.5, fig.width = 5>>=
# filter(stroop.rt, subj == "161832", session == "bas", !is.na(rt))$rt %>% plot(pch = 16)
# filter(stroop.rt, subj == "161832", session == "pro", !is.na(rt))$rt %>% plot(pch = 16)
# filter(stroop.rt, subj == "161832", session == "rea", !is.na(rt))$rt %>% plot(pch = 16)
# filter(stroop.rt, subj == "849971", session == "bas", !is.na(rt))$rt %>% plot(pch = 16)
# filter(stroop.rt, subj == "849971", session == "pro", !is.na(rt))$rt %>% plot(pch = 16)
# filter(stroop.rt, subj == "849971", session == "rea", !is.na(rt))$rt %>% plot(pch = 16)
for (session.i in sessions) {
  p <- stroop.rt %>%
      filter(session == session.i, subj %in% c("849971", "161832")) %>%
      full_join(group_by(., subj) %>% summarize(r2norm = qqr2(rt)), by = "subj") %>%
      ggplot(aes(sample = rt)) +
      stat_qq(alpha = 0.3, size = 0.15) +
      stat_qq_line(size = 0.25) +
      facet_wrap(~subj) +
      geom_text(
        aes(
          x = -1.25, y = 2000, 
          label = paste0("r^2 = ", round(r2norm, 3))
          ), size = 1.75, color = "grey50"
      ) +
      theme_minimal(base_size = 5) +
      theme(axis.title = element_blank()) +
      labs(
        title    = paste0("QQ rt versus normal ", session.i)
          )
      print(p)
}
@



\subsubsection{testing for canonical cognitive control effects}

I'll now test whether robust cognitive effects can be seen within these data.
I'll look across all sessions, and within each.
I'll just dive in, and save the model diagnostics for the next section.
If the effects are discrepant from expectations, I'll perform again after model diagnostics.\\


\noindent \textbf{post-error slowing.}
<<rt-pes, fig.height = 5, results = TRUE, echo = TRUE>>=
# with(stroop, table(subj, session))  ## things seem to be balanced wrt trial numbers.
m.pes <- lmer(rt ~ post.error + (1 | subj), stroop.rt)
m.pes.bas <- update(m.pes, . ~ ., subset = session == "bas")
m.pes.pro <- update(m.pes, . ~ ., subset = session == "pro")
m.pes.rea <- update(m.pes, . ~ ., subset = session == "rea")
summary(m.pes) %>% coef
summary(m.pes.bas) %>% coef
summary(m.pes.pro) %>% coef
summary(m.pes.rea) %>% coef
@

\noindent \textbf{congruency-sequence effect.}
<<rt-cse, fig.height = 5, results = TRUE, echo = TRUE>>=
# with(stroop, table(paste0(trial.type, n1.trial.type), session))  ## things seem to be balanced wrt trial numbers.
m.cse <- lmer(rt ~ n1.trial.type * trial.type + (1 | subj), stroop.rt)
m.cse.bas <- update(m.cse, . ~ ., subset = session == "bas")
m.cse.pro <- update(m.cse, . ~ ., subset = session == "pro")
m.cse.rea <- update(m.cse, . ~ ., subset = session == "rea")
summary(m.cse) %>% coef
summary(m.cse.bas) %>% coef
summary(m.cse.pro) %>% coef
summary(m.cse.rea) %>% coef
@
an aside: is there variability in cse acros subjects?
<<rt-cse-subjvar>>=
## side note: is the cse variable across subjects in proactive? baseline?
m.cse.me <- lmer(rt ~ n1.trial.type * trial.type + (trial.type + n1.trial.type| subj), stroop.rt, control = lmer.cl)
m.cse.bas.me <- update(m.cse.me, . ~ ., subset = session == "bas")
m.cse.pro.me <- update(m.cse.me, . ~ ., subset = session == "pro")
m.cse.rea.me <- update(m.cse.me, . ~ ., subset = session == "rea")
m.cse.re <- lmer(rt ~ n1.trial.type * trial.type + (trial.type * n1.trial.type| subj), stroop.rt, control = lmer.cl)
m.cse.bas.re <- update(m.cse.re, . ~ ., subset = session == "bas")
m.cse.pro.re <- update(m.cse.re, . ~ ., subset = session == "pro")
m.cse.rea.re <- update(m.cse.re, . ~ ., subset = session == "rea")
anova(m.cse.re, m.cse.me)
anova(m.cse.bas.re, m.cse.bas.me)
anova(m.cse.pro.re, m.cse.pro.me)
anova(m.cse.rea.re, m.cse.rea.me)
@
no.

\noindent \textbf{lwpc effect.}
<<rt-lwpc, fig.height = 5, results = TRUE, echo = TRUE>>=
# stroop.rt$item.type <- ifelse(stroop.rt$pc %in% c("mc", "mi"), "bias", "pc50")
m.lwpc <- lmer(
  rt ~ session * trial.type + (1 | subj),
  stroop.rt, 
  subset = session %in% c("bas", "pro") & pc == "pc50"
  )
summary(m.lwpc) %>% coef
@

\paragraph{thoughts.} effects all seem to be there, although I didn't rigourously test them. Good for now, though.


\subsection{errors}


<<error-session-bars, fig.height = 4.5>>=
stroop.er %>%
  group_by(subj, session) %>%
  transmute(error = mean(acc.final %in% c("0", "no.response"))) %>%
  melt(id.vars = c("subj", "session")) %>%
  ggplot(aes(x = subj, y = value, fill = variable)) +
  geom_col(position = position_dodge(width = 1.25), width = 0.75, color = "black") +
  scale_fill_brewer(type = "qual", palette = 1) +
  facet_wrap(~ session, ncol = 1) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), 
    axis.ticks.x =  element_line(),
    legend.position = "none"
    ) +
  labs(title = "error rates")
@


<<error-session-by-type, fig.height = 4.5>>=
stroop.er %>%
  group_by(subj, session) %>%
    transmute(
    p.com = mean(acc.final == "0"),
    p.omi = mean(acc.final == "no.response")
  ) %>%
  melt(id.vars = c("subj", "session")) %>%
  ggplot(aes(x = subj, y = value, fill = variable)) +
  geom_col(position = position_dodge(width = 0.5), width = 0.5, color = "black") +
  scale_fill_brewer(type = "qual", palette = 1) +
  facet_wrap(~ session, ncol = 1) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), 
    axis.ticks.x =  element_line(),
    legend.position = "none"
    ) +
  labs(title = "commission and omission error rates (omission in purple)")
@

Two subjs have particularly high rates of omission: 197449\textunderscore baseline and 160830\textunderscore reactive.
Likely falling asleep.
I'll remove those subject\textunderscore sessions from further error and RT analyses.

<<>>=
is.somnolent.er <- stroop.er$subj == "197449" & stroop.er$session == "bas" |
  stroop.er$subj == "160830" & stroop.er$session == "rea" 
is.somnolent.rt <- stroop.rt$subj == "197449" & stroop.rt$session == "bas" |
  stroop.rt$subj == "160830" & stroop.rt$session == "rea" 
stroop.er <- stroop.er[!is.somnolent.er, ]
stroop.rt <- stroop.rt[!is.somnolent.rt, ]
@

\subsubsection{testing for canonical cognitive control effects}

\noindent \textbf{post-error slowing.}
<<error-pes, fig.height = 5, results = TRUE, echo = TRUE>>=
m.pes.er <- glmer(error ~ post.error + (1 | subj), stroop.er, family = "binomial")
m.pes.bas.er <- update(m.pes.er, . ~ ., subset = session == "bas")
m.pes.pro.er <- update(m.pes.er, . ~ ., subset = session == "pro")  ## did not converge
m.pes.rea.er <- update(m.pes.er, . ~ ., subset = session == "rea")  ## did not converge
summary(m.pes.er) %>% coef
summary(m.pes.bas.er) %>% coef
summary(m.pes.pro.er) %>% coef
summary(m.pes.rea.er) %>% coef
@

\noindent \textbf{congruency-sequence effect.}
<<error-cse, fig.height = 5, results = TRUE, echo = TRUE>>=
# with(stroop, table(paste0(trial.type, n1.trial.type), session))  ## things seem to be balanced wrt trial numbers.
m.cse <- lmer(rt ~ n1.trial.type * trial.type + (1 | subj), stroop.rt)
m.cse.bas <- update(m.cse, . ~ ., subset = session == "bas")
m.cse.pro <- update(m.cse, . ~ ., subset = session == "pro")
m.cse.rea <- update(m.cse, . ~ ., subset = session == "rea")
summary(m.cse) %>% coef
summary(m.cse.bas) %>% coef
summary(m.cse.pro) %>% coef
summary(m.cse.rea) %>% coef
@

\noindent \textbf{lwpc effect.}
<<error-lwpc, fig.height = 5, results = TRUE, echo = TRUE>>=
# stroop.rt$item.type <- ifelse(stroop.rt$pc %in% c("mc", "mi"), "bias", "pc50")
m.lwpc.er <- glmer(
  error ~ session * trial.type + (1 | subj),
  stroop.er, 
  subset = session %in% c("bas", "pro") & pc == "pc50",
  family = "binomial"
  )
summary(m.lwpc.er)
@

\paragraph{thoughts.} 
the directions are all there. 
but the effects and models are generally less robust, likely bc of few errors.
combining performance across sessions may lead to better estimates.


\section{model building}

<<>>=

stroop.rt <- stroop.rt %>% filter(rt < 3000, rt > 250)
# plot(stroop.rt$rt)

counts.subj.session <- table(stroop.rt$subj, stroop.rt$session) %>% as.data.frame
subjs.to.rm.bas <- counts.subj.session %>% filter(Var2 %in% c("bas", "pro"), Freq < 1) %>% .$Var1 %>% as.character
subjs.to.rm.all <- counts.subj.session %>% filter(Var2 %in% c("bas", "pro", "rea"), Freq < 1) %>% .$Var1 %>% as.character

length(group201902.analysis) - sum(subjs.to.rm.bas %in% group201902.analysis)  ## sample size with pro & bas
length(group201902.analysis) - sum(subjs.to.rm.all %in% group201902.analysis)  ## sample size with all

## for now, let's use the reduced set:

stroop.rt.all <- filter(stroop.rt, !subj %in% subjs.to.rm.all)
stroop.er.all <- filter(stroop.er, !subj %in% subjs.to.rm.all)

@

These models are fit on the sample of subjects who have completed ALL sessions, and are in the RSA group sample.
A total of \Sexpr{length(stroop.rt.all$subj)} subjects: \Sexpr{unique(stroop.rt.all$subj)}


\subsection{assessing amount of variance in stroop effect across subjects}

\subsubsection{response time}

<<rt-stroop-subj-var-mods, echo = TRUE, results = TRUE>>=

## models:

mrt.null <- lmer(rt ~ trial.type + (1 | subj), stroop.rt.all)
mrt.base <- lmer(rt ~ trial.type + (trial.type | subj), stroop.rt.all)
mrt.null.bas <- lmer(rt ~ trial.type + (1 | subj), stroop.rt.all %>% filter(session == "bas"))
mrt.null.pro <- lmer(rt ~ trial.type + (1 | subj), stroop.rt.all %>% filter(session == "pro"))
mrt.null.rea <- lmer(rt ~ trial.type + (1 | subj), stroop.rt.all %>% filter(session == "rea"))
mrt.base.bas <- lmer(rt ~ trial.type + (trial.type | subj), stroop.rt.all %>% filter(session == "bas"))
mrt.base.pro <- lmer(rt ~ trial.type + (trial.type | subj), stroop.rt.all %>% filter(session == "pro"))
mrt.base.rea <- lmer(rt ~ trial.type + (trial.type | subj), stroop.rt.all %>% filter(session == "rea"))

## percent variance reduced:

per.var.reduction(mrt.null, mrt.base)  ## overall
per.var.reduction(mrt.null.bas, mrt.base.bas)  ## baseline
per.var.reduction(mrt.null.pro, mrt.base.pro)  ## proactive
per.var.reduction(mrt.null.rea, mrt.base.rea)  ## reactive

## significance tests:

anova(mrt.null, mrt.base)  ## all are sig.
anova(mrt.null.bas, mrt.base.bas)
anova(mrt.null.pro, mrt.base.pro)
anova(mrt.null.rea, mrt.base.rea)

@

\paragraph{thoughts.}
there is some variance to work with in RTs: most-so in bas; least-so in pro.

\subsubsection{errors}


<<er-stroop-subj-var-mods, echo = TRUE, results = TRUE, tidy = TRUE>>=
## models:

mer.null <- glmer(error ~ trial.type + (1 | subj), stroop.er.all, family = "binomial")
mer.base <- glmer(error ~ trial.type + (trial.type | subj), stroop.er.all, family = "binomial")
mer.null.bas <- glmer(error ~ trial.type + (1 | subj), stroop.er.all %>% filter(session == "bas"), family = "binomial")
mer.null.pro <- glmer(
  error ~ trial.type + (1 | subj), stroop.er.all %>% filter(session == "pro"), family = "binomial", control = glmer.cl
  )
mer.null.rea <- glmer(
  error ~ trial.type + (1 | subj), stroop.er.all %>% filter(session == "rea"), family = "binomial",
  control = glmer.cl
  )
mer.base.bas <- glmer(error ~ trial.type + (trial.type | subj), stroop.er.all %>% filter(session == "bas"), family = "binomial")
mer.base.pro <- glmer(
  error ~ trial.type + (trial.type | subj), stroop.er.all %>% filter(session == "pro"), family = "binomial",
  control = glmer.cl
  )
mer.base.rea <- glmer(error ~ trial.type + (trial.type | subj), stroop.er.all %>% filter(session == "rea"), family = "binomial")

## percent variance reduced:
# 
# per.var.reduction(mer.null, mer.base)  ## overall
# per.var.reduction(mer.null.bas, mer.base.bas)  ## baseline
# per.var.reduction(mer.null.pro, mer.base.pro)  ## proactive
# per.var.reduction(mer.null.rea, mer.base.rea)  ## reactive

## significance tests:

anova(mer.null, mer.base)  ## sig.
anova(mer.null.bas, mer.base.bas)  ## ns.
anova(mer.null.pro, mer.base.pro)  ## ns.
anova(mer.null.rea, mer.base.rea)  ## sig

## now for bas and pro alone:
mer.null.baspro <- update(mer.null, . ~ ., subset = session %in% c("bas", "pro"))
mer.base.baspro <- update(mer.base, . ~ ., subset = session %in% c("bas", "pro"), control = glmer.cl)
anova(mer.null.baspro, mer.base.baspro)  ## the variance across subjects is pretty weak, even in bas and pro combined.

@

\paragraph{thoughts.}
overall, there's variance to work with; within each session separately, however, there is not.
Seems like a fair chunk of the variance may come from reactive session.

\subsection{assessing the interaction between subject variance in stroop and session}

I'll only attempt this with response times.
I dont' think our data will handle

\subsubsection{response time}

<<rt-stroo-subj-varX, echo = TRUE, results = TRUE, tidy = TRUE>>=

## get main effect model:
## NB: models wouldnt converge with fixed effect interaction
mrt.subj.stroop.x.ses0 <- lmer(rt ~ trial.type + session + (trial.type | subj), stroop.rt.all, control = lmer.cl)
mrt.subj.stroop.x.ses1 <- lmer(rt ~ trial.type + session + (trial.type + session | subj), stroop.rt.all, control = lmer.cl)
mrt.subj.stroop.x.ses2 <- lmer(rt ~ trial.type + session + (trial.type | subj) + (session | subj), stroop.rt.all, control = lmer.cl)
anova(mrt.subj.stroop.x.ses0, mrt.subj.stroop.x.ses1, mrt.subj.stroop.x.ses2)
summary(mrt.subj.stroop.x.ses2)
# summary(mrt.subj.stroop.x.ses1)

## get interaction model:
mrt.subj.stroop.x.ses <- lmer(
  rt ~ trial.type + session + (trial.type | subj) * (session | subj) + (session:trial.type | subj), stroop.rt.all,
  control = lmer.cl, REML = FALSE
  )
summary(mrt.subj.stroop.x.ses)

## do subject differences in stroop effect change with session?
anova(mrt.subj.stroop.x.ses, mrt.subj.stroop.x.ses2)

## now for just bas and pro alone:
mrt.subj.stroop.x.ses.baspro <- lmer(
  rt ~ trial.type * session + (trial.type | subj) + (session | subj), stroop.rt.all, control = lmer.cl,
  subset = session == c("bas", "pro")
  )
mrt.subj.stroop.x.ses.baspro2 <- lmer(
  rt ~ trial.type * session + (trial.type | subj) + (session | subj) + (session:trial.type | subj), stroop.rt.all, control = lmer.cl,
  subset = session == c("bas", "pro")
  )
anova(mrt.subj.stroop.x.ses.baspro, mrt.subj.stroop.x.ses.baspro2)

@


\paragraph{thoughts.}
Seems as if the variance between session to session in subject's stroop effects in RT is marginal at best and dispreferred in terms of model parsimony (BIC).


\subsection{assessing within-subject reliability of stroop effect}

\subsubsection{response time}

\paragraph{excluding addtional points.}

RTs with $3.5 > SD$ from mean of each subject removed.
Hopefully will add stability to ranef slope estimates.

<<rt-outlier-exlusion, fig.height = 5>>=

qqn <- function(x) {setNames(qqnorm(x, plot.it = FALSE), c("theoretical", "sample"))}
stroop.rt <- stroop.rt %>%
    group_by(subj, session) %>%
    mutate(is.sd3.5 = abs(rt - mean(rt)) > 3.5 * sd(rt))
stroop.rt.all <-stroop.rt.all %>%
    group_by(subj, session) %>%
    mutate(is.sd3.5 = abs(rt - mean(rt)) > 3.5 * sd(rt))

for (session.i in sessions) {
  p <- stroop.rt %>%
    filter(session == session.i) %>%
    full_join(group_by(., subj) %>% summarize(r2norm = qqr2(rt))) %>%
    group_by(subj) %>%
    mutate(
      sample = qqn(rt)$sample,
      theoretical = qqn(rt)$theoretical,
      xtreme = ifelse(is.sd3.5, "raw", "none")
    ) %>%
    ggplot(aes(theoretical, sample)) +
    geom_point(aes(alpha = xtreme, color = xtreme), size = 0.15) +
    # geom_point(aes(alpha = is.sd3.5, color = is.sd3.5), size = 0.15) +
    scale_color_manual(values = c(none = "black", raw = "#e41a1c", inv = "#1f78b4")) +
    scale_alpha_manual(values = c(none = 0.3, raw = 0.9, inv = 0.9)) +
    facet_wrap(~subj) +
    geom_text(
      aes(
        x = -1.25, y = 2000,
        label = paste0("r^2 = ", round(r2norm, 3))
        ), size = 1.75, color = "grey50"
    ) +
    theme_minimal(base_size = 5) +
    theme(axis.title = element_blank(), legend.position = "none") +
    labs(
      title    = paste0("QQ rt versus normal: ", session.i),
      subtitle = paste0("overall r^2 with normal = ", round(qqr2(stroop.rt$rt), 3))
        )
  print(p)
}
outs <- stroop.rt$is.sd3.5
total <- sum(outs)
@




<<>>=

mrt.stroop.pro.full <- lmer(
  rt ~ trial.type + (trial.type | subj), stroop.rt,
  subset = !is.sd3.5 & session == "pro"
  )
mrt.stroop.pro <- lmer(
  rt ~ trial.type + (trial.type | subj), stroop.rt,
  subset = !is.sd3.5 & session == "pro" & subj %in% group201902.analysis,
  control = lmer.cl
  )
mrt.stroop.baspro <- lmer(
  rt ~ trial.type + (trial.type | subj), stroop.rt,
  subset = !is.sd3.5 & session %in% c("pro", "bas") & !subj %in% subjs.to.rm.bas
  )


summary(mrt.stroop.pro)
summary(mrt.stroop.baspro)

baspro <- ranef(mrt.stroop.baspro)$subj %>%
    tibble::rownames_to_column("subj") %>%
    rename(con = "(Intercept)", stroop = trial.typei)
pro.full <- ranef(mrt.stroop.pro.full)$subj %>%
    tibble::rownames_to_column("subj") %>%
    rename(con = "(Intercept)", stroop = trial.typei)
pro <- ranef(mrt.stroop.pro)$subj %>%
    tibble::rownames_to_column("subj") %>%
    rename(con = "(Intercept)", stroop = trial.typei)

lmer.coefs <- inner_join(
  pro %>% select(stroop.pro = stroop, subj),
  pro.full %>% select(stroop.pro.full = stroop, subj),
  by = "subj"
)
lmer.coefs <- inner_join(lmer.coefs, baspro %>% select(stroop.baspro = stroop, subj))
lmer.coefs %>% select(-subj) %>% cor(method = "spearman")
var(lmer.coefs$stroop.pro.full)
var(lmer.coefs$stroop.pro)
var(lmer.coefs$stroop.baspro)
@



% 
% 
% \section{model building SCRATCH}
% 
% Reducing the level 1 error term will afford greater precision, reliability, and size of level 2 parameter estimates.
% Here's a list of covariates that I'll try to model:
% \begin{itemize}
%   \item time-on-task effects: trial number, block number, etc (and potential nonlinear effects)
%   \item session effects
%   \item item group effects (PC)
%   \item previous trial RT---autoregressive components
%   \item response / item effects (i.e., color name)
% \end{itemize}
% I'll also consider alternative models (e.g., inverse gaussian, gamma).
% 
% I'll criticize each model along the way (via diagnostic plots).
% \subsection{response times}
% 
% <<>>=
% stroop.rt.complete <- filter(stroop.rt.complete, rt > 250)
% 
% icc.stroop.subj <- function(mod) {
%   vc <- as.data.frame(VarCorr(mod))
%   var.subj  <- vc[vc$grp == "subj" & vc$var1 == "trial.typei", "vcov"]
%   var.resid <- vc[vc$grp == "Residual", "vcov"]
%   var.subj / (var.resid + var.subj)
% }
% 
% icc.stroop.subj(m.gaus.rt4)
% icc.stroop.subj(VarCorr(m.gaus.rt0))
% 
% 
% install.packages("rms")
% stroop.rt.complete <- stroop.rt.complete %>%
%   group_by(subj, session, run) %>%
%   arrange(subj, session, run, trial.num) %>%
%   mutate(prev.rt = lag(rt)) %>%
%   group_by(subj) %>%
%   mutate(prev.rt = scale(prev.rt, scale = FALSE))
% gm1 <- glmer(
%   rt ~ trial.type + trial.num + prev.rt + (trial.type | subj), stroop.rt.complete, subset = !is.na(prev.rt),
%   control = glmer.cl,
%   family = inverse.gaussian(link = "identity")
%   )
% m1 <- lmer(rt ~ trial.type + (trial.type | subj), stroop.rt.complete, subset = !is.na(prev.rt), control = cl)
% m2a <- update(m1, . ~ . + rms::rcs(trial.num))
% m2b <- update(m1, . ~ . + trial.num)
% m3 <- update(m1, . ~ . + prev.rt)
% m4 <- update(m2b, . ~ . + prev.rt)
% anova(m1, m2a, m2b, m3, m4)
% plot(m2b)
% plot(m1)
% plot(m3)
% plot(m4)
% 
% m5 <- lmer(
%   rt ~ trial.type * session + trial.num + prev.rt + (trial.type | subj),
%   stroop.rt.complete, 
%   subset = !is.na(prev.rt), control = cl
%   )
% anova(m4, m5)
% qqnorm(resid(gm1))
% qqnorm(resid(m5))
% 
% m5 <- update(m4, . ~ . + session)
% m6  <- update(m5, . ~ . + ())
% plot(m5)
% 
% 
% plot(gm1)
% 
% 
% r.squaredGLMM(gm1)
% r.squaredLM(gm1)
% 
% 
% 
% 
% ## ---
% library(nlme)
% cl1 <- lmeControl(
%   maxIter = 100000, msMaxIter = 100000, niterEM = 100000,
%   msMaxEval = 100000, tolerance = 0.000001, msTol = 0.0000001, returnObject = TRUE,
%   minAbsParApVar = 0.05, opt = c("nlminb"), optimMethod = "BFGS"
%   )
% cl2 <- lmeControl(
%   maxIter = 100000, msMaxIter = 100000, niterEM = 100000,
%   msMaxEval = 100000, tolerance = 0.000001, msTol = 0.0000001, returnObject = TRUE,
%   minAbsParApVar = 0.05, opt = c("optim"), optimMethod = "BFGS"
%   )
% cl3 <- lmeControl(
%   maxIter = 100000, msMaxIter = 100000, niterEM = 100000,
%   msMaxEval = 100000, tolerance = 0.000001, msTol = 0.0000001, returnObject = TRUE,
%   minAbsParApVar = 0.05, opt = c("optim"), optimMethod = "L-BFGS-B"
%   )
% f.lme <- formula(rt ~ trial.type)
% m.samevar <- lme(f.lme, random = ~ 1 + trial.type | subj, data = stroop.rt.complete, control = cl1)
% summary(m.samevar)
% m.diffvar <- update(m.samevar, . ~ ., correlation = varIdent(form = ~ 1 | subj))
% summary(m.diffvar)
% anova(m.samevar, m.diffvar)  ## variances are different. duh.
% 
% ## ---
% 
% 
% 
% 
% 
% 
% 
% 
% install.packages("MuMIn")
% library(MuMIn)
% 
% ## all sessions:
% 
% m.gaus.rt0 <- lmer(rt ~ trial.type + (1 | subj), stroop.rt.complete, control = cl)
% m.gaus.rt1 <- update(m.gaus.rt0, . ~ . + (1 | color))
% m.gaus.rt2 <- lmer(rt ~ trial.type + (trial.type | subj), stroop.rt.complete, control = cl)
% m.gaus.rt3 <- update(m.gaus.rt2, . ~ . + (1 | color))
% m.gaus.rt4 <- update(m.gaus.rt2, . ~ . + (trial.type | color))
% anova(m.gaus.rt0, m.gaus.rt2)  ## sig variance across subjs
% anova(m.gaus.rt0, m.gaus.rt1)  ## sig variance across item RTs
% anova(m.gaus.rt2, m.gaus.rt3)  ## ...
% anova(m.gaus.rt3, m.gaus.rt4)  ## sig variance across item stroop effs.
% 
% 
% m.gaus.rt0.b <- update(m.gaus.rt0, subset = session == "bas")
% m.gaus.rt1.b <- update(m.gaus.rt1, subset = session == "bas")
% m.gaus.rt2.b <- update(m.gaus.rt2, subset = session == "bas")
% m.gaus.rt3.b <- update(m.gaus.rt3, subset = session == "bas")
% m.gaus.rt4.b <- update(m.gaus.rt4, subset = session == "bas")
% anova(m.gaus.rt0.b, m.gaus.rt2.b)  ## sig variance across subjs
% anova(m.gaus.rt0.b, m.gaus.rt1.b)  ## sig variance across item RTs
% anova(m.gaus.rt2.b, m.gaus.rt3.b)  ## ...
% anova(m.gaus.rt3.b, m.gaus.rt4.b)  ## sig variance across item stroop effs.
% 
% m.gaus.rt0.p <- update(m.gaus.rt0, subset = session == "pro")
% m.gaus.rt1.p <- update(m.gaus.rt1, subset = session == "pro")
% m.gaus.rt2.p <- update(m.gaus.rt2, subset = session == "pro")
% m.gaus.rt3.p <- update(m.gaus.rt3, subset = session == "pro")
% m.gaus.rt4.p <- update(m.gaus.rt4, subset = session == "pro")
% anova(m.gaus.rt0.p, m.gaus.rt2.p)  ## sig variance across subjs
% anova(m.gaus.rt0.p, m.gaus.rt1.p)  ## sig variance across item RTs
% anova(m.gaus.rt2.p, m.gaus.rt3.p)  ## ...
% anova(m.gaus.rt3.p, m.gaus.rt4.p)  ## sig variance across item stroop effs.
% 
% m.gaus.rt0.r <- update(m.gaus.rt0, subset = session == "rea")
% m.gaus.rt1.r <- update(m.gaus.rt1, subset = session == "rea")
% m.gaus.rt2.r <- update(m.gaus.rt2, subset = session == "rea")
% m.gaus.rt3.r <- update(m.gaus.rt3, subset = session == "rea")
% m.gaus.rt4.r <- update(m.gaus.rt4, subset = session == "rea")
% anova(m.gaus.rt0.r, m.gaus.rt2.r)  ## sig variance across subjs
% anova(m.gaus.rt0.r, m.gaus.rt1.r)  ## sig variance across item RTs
% anova(m.gaus.rt2.r, m.gaus.rt3.r)  ## ...
% anova(m.gaus.rt3.r, m.gaus.rt4.r)  ## sig variance across item stroop effs.r
% 
% 
% m.invg.rt0 <- glmer(rt ~ trial.type + (1 | subj), stroop.rt.complete, control = glmer.cl, family = inverse.gaussian(link = "identity"))
% m.invg.rt1 <- update(m.invg.rt0, . ~ . + (1 | color))
% m.invg.rt2 <- glmer(
%   rt ~ trial.type + (trial.type | subj),
%   stroop.rt.complete, control = glmer.cl, family = inverse.gaussian(link = "identity")
%   )
% m.invg.rt3 <- update(m.invg.rt2, . ~ . + (1 | color))
% m.invg.rt4 <- update(m.invg.rt2, . ~ . + (trial.type | color))
% anova(m.invg.rt0, m.invg.rt2)  ## sig variance across subjs
% anova(m.invg.rt0, m.invg.rt1)  ## sig variance across item RTs
% anova(m.invg.rt2, m.invg.rt3)  ## ...
% anova(m.invg.rt3, m.invg.rt4)  ## sig variance across item stroop effs.
% 
% m.invg.rt0.p <- update(m.invg.rt0, subset = session == "pro")
% m.invg.rt1.p <- update(m.invg.rt1, subset = session == "pro")
% m.invg.rt2.p <- update(m.invg.rt2, subset = session == "pro")
% m.invg.rt3.p <- update(m.invg.rt3, subset = session == "pro")
% m.invg.rt4.p <- update(m.invg.rt4, subset = session == "pro")
% anova(m.invg.rt0.p, m.invg.rt2.p)  ## no sig var across subjs
% anova(m.invg.rt0.p, m.invg.rt1.p)  ## sig variance across item RTs
% anova(m.invg.rt2.p, m.invg.rt3.p)  ## ...
% anova(m.invg.rt3.p, m.invg.rt4.p)  ## sig variance across item stroop effs.r
% 
% 
% 
% 
% 
% 
% 
% ## exclude all subjects with 0 trials for any session
% tab <- as.data.frame(table(stroop.rt$subj, stroop.rt$session))
% subjs.incomplete <- as.character(tab[tab$Freq < 1, "Var1"])
% stroop.rt.complete <- filter(stroop.rt, !subj %in% tab)
% 
% cl <- lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))  ## increase num iteration
% 
% m.rt.empty <- lmer(rt ~ 1 + (1 | subj), stroop.rt.complete)
% m.rt1 <- lmer(rt ~ trial.type + (1 | subj), stroop.rt.complete)
% m.rt2 <- lmer(rt ~ trial.type + (1 | subj) + (1 | color), stroop.rt.complete)
% m.rt3 <- lmer(rt ~ trial.type + (trial.type | subj) + (1 | color), stroop.rt.complete, control = cl)
% m.rt4 <- lmer(rt ~ trial.type + (1 | subj) + (trial.type | color), stroop.rt.complete)
% m.rt5 <- lmer(rt ~ trial.type + (trial.type | subj) + (trial.type | color), stroop.rt.complete)
% 
% 
% m.rt6 <- lmer(
%   rt ~ trial.type + session + (trial.type | subj) + (trial.type | color),
%   stroop.rt.complete, control = cl
%   )
% m.rt7 <- lmer(
%   rt ~ trial.type + session + (trial.type + session | subj) + (trial.type | color),
%   stroop.rt.complete, control = cl
%   )
% 
% 
% anova(m.rt.empty, m.rt1, m.rt2, m.rt3, m.rt4, m.rt5, m.rt6, m.rt7)
% 
% plot(m.rt.empty)
% plot(m.rt1)
% plot(m.rt2)
% plot(m.rt5)
% 
% plot(m.rt7)
% 
% 
% m.rt6 <- lmer(rt ~ trial.type + session + (trial.type | subj) + (trial.type | color), stroop.rt)
% 
% # 
% #   family = inverse.gaussian(link = "identity"),
% #   ## increase num iter:
% #   glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))
% #   )
% # fit.gam <- glmer(
% #  rt ~ type + (type | subj) + (type | color), rt1,
% #  family = Gamma(link = "identity"),
% #  glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))
% 
% 
% glmer.cl <- glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))
% gam.m.rt.empty <- glmer(rt ~ 1 + (1 | subj), stroop.rt.complete, family = Gamma(link = "identity"), control = glmer.cl)
% gam.m.rt1 <- update(gam.m.rt1, . ~ . + trial.type)
% 
% ing.m.rt.empty <- glmer(rt ~ 1 + (1 | subj), stroop.rt.complete, family = inverse.gaussian(link = "identity"), control = glmer.cl)
% ing.m.rt1 <- update(ing.m.rt.empty, . ~ . + trial.type)
% ing.m.rt2 <- update(ing.m.rt1, . ~ . +  (1 | color))
% ing.m.rt3 <- glmer(
%   rt ~ trial.type + (trial.type | subj),
%   stroop.rt.complete, 
%   family = inverse.gaussian(link = "identity"), control = glmer.cl
%   )
% ing.m.rt4 <- update(ing.m.rt1, . ~ . +  (trial.type | color))
% ing.m.rt5 <- update(ing.m.rt3, . ~ . +  (trial.type | color))
% 
% anova(ing.m.rt.empty, ing.m.rt1, ing.m.rt2, ing.m.rt3, ing.m.rt4)
% 
% anova(ing.m.rt1, ing.m.rt3)
% 
% plot(ing.m.rt1)
% plot(gam.m.rt1)
% 
% 
% 
% 
% 
% 
% 
% resid(m.rt7)
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% library(qqplotr)
% ggplot(stroop.rt.complete, aes(sample = scale(resid(m.rt7)))) + 
%   stat_qq_band() +
%   stat_qq_line() + 
%   stat_qq_point()
% 
% L1_Fitted <- fitted(HSB_Fit_1)
% L1_residuals <- resid(HSB_Fit_1)
% HSB_Data_2 <- cbind(HSB_Data, L1_Fitted, L1_residuals)
% names(HSB_Data_2)
% 
% ggplot(HSB_Data_2, aes(x = L1_Fitted, y = L1_residuals)) + geom_point(shape = 19,
% size = 1, color = "blue", na.rm = TRUE, alpha = 0.2) + geom_smooth(method = loess,
% se = FALSE, color = "red", size = 1.5, linetype = 1, na.rm = TRUE) +
% scale_y_continuous(breaks = c(seq(-20, 20, 5))) + scale_x_continuous(breaks = c(seq(-1,
% 22, 3))) + coord_cartesian(xlim = c(-1, 22), ylim = c(-20, 20)) +
% xlab("Fitted Value") + ylab("Level 1 Residual")
% @


<<>>=
# glm()

@



% 
% 
% 
%   facet_wrap(~subj) +
%   geom_text(
%     aes(
%       x = -1.25, y = 2000, 
%       label = paste0("r^2 = ", round(r2norm, 3))
%       ), size = 1.75, color = "grey50"
%   ) +
%   theme_bw(base_size = 5) +
%   theme(axis.title = element_blank()) +
%   labs(
%     title    = "QQ rt versus normal",
%     subtitle = paste0("overall r^2 with normal = ", round(qqr2(stroop.samp.rt$rt), 3))
%       )
% @
% 


% 
% \subsection{response times}
% 
% \subsubsection
% 
% \vspace{12}
% 
% bimodal, with smaller mode around $\sim 500$ ms:
%   
%   \vspace{12}
% 
% <<rt-session-hists, fig.width = 6, fig.height = 2>>=
%   stroop.rt %>%
%   ggplot(aes(rt)) +
%   geom_density(fill = "slateblue", alpha = 0.8) +
%   # geom_histogram(aes(fill = session), color = "grey30", size = 0.1, binwidth = 20) +
%   facet_wrap(~ session) +
%   # scale_fill_brewer(type = "qual", palette = 2) +
%   labs(title = "all correct trials")
% @
%   
%   \vspace{12}
% 
% \noindent the ``mark rothko'' pattern:
%   
%   \vspace{12}
% 
% <<rt-dots, fig.width = 6, fig.height = 2>>=
%   stroop.rt %>%
%   ggplot(aes(pc, rt)) +
%   geom_point(
%     aes(color = trial.type), fill = "white", position = position_jitterdodge(jitter.width = 0.3),
%     size = 0.01, alpha = 0.1
%   ) +
%   scale_color_manual(values = color_congruency) +
%   labs(title = "all sessions")
% @
%   
%   each dot is a trial; all subjects, sessions; red is incongruent, blue is congruent
% 
% \vspace{48}
% 
% \noindent next pages are by subject and session.\\
% 
% this pattern seems like an artifact to me because it's so consistent across sessions, (some) subjects, and task conditions.
% 
% \vspace{12}
% 
% 
% <<rt-dots-baseline, fig.height = 10>>=
% stroop.rt %>%
% ggplot(aes(pc, rt)) +
% geom_point(
% aes(fill = trial.type, color = trial.type), position = position_jitterdodge(),
% size = 0.1, alpha = 0.3
% ) +
% facet_wrap(~ subj, ncol = 6) +
% theme(axis.title = element_blank(), legend.position = "none") +
% scale_color_manual(values = color_congruency) +
% labs(title = "baseline")
% @
% 
% 
% <<rt-dots-proactive, fig.height = 10>>=
% stroop.rt %>%
% ggplot(aes(pc, rt)) +
% geom_point(
% aes(fill = trial.type, color = trial.type), position = position_jitterdodge(),
% size = 0.1, alpha = 0.3
% ) +
% facet_wrap(~ subj, ncol = 6) +
% theme(axis.title = element_blank(), legend.position = "none") +
% scale_color_manual(values = color_congruency) +
% labs(title = "proactive")
% @
% 
% <<rt-dots-reactive, fig.height = 10>>=
% stroop.rt %>%
% ggplot(aes(pc, rt)) +
% geom_point(
% aes(fill = trial.type, color = trial.type), position = position_jitterdodge(),
% size = 0.1, alpha = 0.3
% ) +
% facet_wrap(~ subj, ncol = 6) +
% theme(axis.title = element_blank(), legend.position = "none") +
% scale_color_manual(values = color_congruency) +
% labs(title = "reactive")
% @
% 
% 
% \subsection{Accuracy data}
% 
% Do we believe the accuracy data as reflecting cognitive processes? \\
% 
% What to do about unintelligible responses?
% 
% <<>>=
% outpro <- function(m, stand = FALSE){
% ## adapted from https://doi.org/10.3389/fpsyg.2012.00606
% ## specifically,
% ## from the robustcorr toolbox function detect_outliers.m:
% ## https://github.com/CPernet/robustcorrtool/blob/master/detect_outliers.m
% ## and the WRS2 function outpro.R availale on the github mirror:
% ## https://github.com/cran/WRS2/blob/master/R/outpro.R
% source("C:/local/WRS2/R/idealf.R")  ## for idealf
% m <- as.matrix(m)
% nobs <- nrow(m)
% if(stand) m = scale(m)
% gval <- sqrt(qchisq(0.975, ncol(m)))  ## iqr value (boxplot rule) for chisquare w 2 df
% # library(MASS)
% # center <- cov.mcd(m)$center
% library(robustbase)
% center <- covMcd(m, nsamp = "exact")$center
% is.outlier <- vector("logical", nobs)
% vec <- seq_len(nobs)
% for (i in vec) {
% B <- m[i, ] - center  ## difference vector B (from center to point i)
% dis <- vector("numeric", nobs)
% # dis <- rep(NA, nobs)  ## initialize distances vector
% bot <- c(B %*% B)  ## length^2 of difference vector B
% if (isTRUE(all.equal(bot, 0))) next  ## 0 vector has no length
% for (j in vec){
% A <- m[j, ] - center  ## difference vector A (from center to point j)
% Bp <- A %*% B %*% B / bot  ## scale length of B by projection of A on B
% dis[j] <- vlen(Bp)  ## get length of Bp
% }
% fourths <- idealf(dis)  ## get ideal fourths (?)
% cu <- median(dis) + gval * (fourths$qu - fourths$ql)  ## chisqr cutoff (boxplot rule)
% flag <- dis > cu
% is.outlier[flag] <- TRUE  ## to save results across iterations
% }
% outliers <- vec[is.outlier]
% keep <- vec[!is.outlier]
% list(n.obs = nobs, n.outliers = length(outliers), outliers = outliers, keep = keep)
% }
% skipcor <- function(allx, ally, nboot = 10000, ...) {
% ## for bivariate cases only
% # allx = df$stroop.er.mean
% # ally = df$silhouette
% # if (ncol(allx) > 1 | ncol(ally) > 1) stop("need column vecs")
% allm <- cbind(allx, ally)
% # keep.obs <- outpro(allm)$keep
% # # keep.obs <- outpro(allm, cop = 2, plotit = FALSE)$keep
% # nkeep <- length(keep.obs)
% nkeep <- nrow(allm)
% # print(noquote(paste0("excluded ", nrow(allm) - nkeep, " obs as outliers")))
% x <- allm[, 1]
% y <- allm[, 2]
% rho <- cor(x, y, method = "spearman")
% r <- cor(x, y)
% ev <- c(spearman = rho, pearson = r)
% if (nboot < 1) return(ev)
% samps <- matrix(
% sample.int(nkeep, nkeep * nboot, replace = TRUE),
% nrow = nkeep, ncol = nboot
% )
% dist.r <- apply(samps, 2, function(s) cor(x[s], y[s]))
% dist.rho <- apply(samps, 2, function(s) cor(x[s], y[s], method = "spearman"))
% ## frequentist's p:
%   lb <- (1 / 20 * nboot) / 2
%   ub <- nboot - lb
%   dist.rho <- sort(dist.rho)
%   dist.r <- sort(dist.r)
%   ci <- rbind(
%     spearman = c(lb = dist.rho[lb], ub = dist.rho[ub]),
%     pearson  = c(lb = dist.r[lb], ub = dist.r[ub])
%   )
%   cbind(ev, ci)
% }
% vlen        <- function(v) sqrt(sum(v^2))
% 
% 
% stroop.error.counts <- stroop %>%
%   filter(!is.na(acc.final)) %>%
%   group_by(subj, session) %>%
%   summarize(
%     p.uni = mean(acc.final == "unintelligible"),
%     p.com = mean(acc.final == "0"),
%     p.omi = mean(acc.final == "no.response")
%   ) %>%
%   filter(!subj %in% c("23326", "198855"))
% 
% 
% filter(stroop.error.counts, session == "pro")$p.com %>% mean * 100
% filter(stroop.error.counts, session == "pro")$p.omi %>% mean * 100
% filter(stroop.error.counts, session == "pro")$p.uni %>% mean * 100
% 
% 
% stroop.error.counts.w <- stroop.error.counts %>% 
%   filter(session == "pro") %>%
%   melt %>%
%   dcast(subj ~ variable, value.var = "value")
% 
% ## are somewhat correlated across subjects:
% skcor.unintel.comis <- skipcor(stroop.error.counts.w$p.uni, stroop.error.counts.w$p.com)
% skcor.unintel.omis <- skipcor(stroop.error.counts.w$p.uni, stroop.error.counts.w$p.omi)
% 
% stroop <- stroop %>% filter(!is.na(acc.final))
% stroop$error <- ifelse(stroop$acc.final != 1, 1, 0)
% stroop$acc.final %>% unique
% library(lme4)
% int <- glmer(error ~ trial.type + (1 | subj), stroop, family = "binomial", subset = acc.final %in% c("1", "unintelligible") & session == "pro")
% summary(int)
% int1 <- glmer(error ~ trial.type + (trial.type | subj), stroop, family = "binomial", subset = acc.final %in% c("1", "unintelligible") & session == "pro")
% anova(int, int1)
% 
% cor(stroop.error.counts.w[c("p.uni", "p.com", "p.omi")], method = "spearman")
% 
% plot(stroop.error.counts.w[c("p.uni", "p.com")])
% plot(stroop.error.counts.w[c("p.uni", "p.omi")])
% plot(stroop.error.counts.w[c("p.com", "p.omi")])
% stroop.error.counts.w[which.max(stroop.error.counts.w$p.uni), ]
% 
% stroop %>%
%   filter(!is.na(acc.final)) %>%
%   group_by(subj, session) %>%
%   summarize(
%     p.uni = mean(acc.final == "unintelligible"),
%     p.com = mean(acc.final == "0"),
%     p.omi = mean(acc.final == "no.response")
%   ) %>%
%   melt(id.vars = c("subj", "session")) %>%
%   ggplot(aes(x = subj, y = value, fill = variable)) +
%   geom_col(position = position_dodge(width = 1), width= 0.5) +
%   scale_fill_brewer(type = "qual", palette = 2) +
%   facet_wrap(~ session, ncol = 1, scales = "free") +
%   theme(
%     axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), 
%     axis.ticks.x =  element_line()
%   )
% @
%   
%   Should we focus on errors of comission, omission, or both?\\
% 
% <<>>=
%   ## subject counts:
%   # stroop.er<- stroop %>% filter(!is.na(acc))
%   table(stroop.er$subj, stroop.er$session, stroop.er$acc.final)
% 
% stroop %>%
%   group_by(subj, session) %>%
%   transmute(
%     p.com = mean(acc.final == "0"),
%     p.omi = mean(acc.final == "no.response")
%   ) %>%
%   melt(id.vars = c("subj", "session")) %>%
%   ggplot(aes(x = subj, y = value, fill = variable)) +
%   geom_col(position = position_dodge(width = 1.25), width = 0.75, color = "black") +
%   scale_fill_brewer(type = "qual", palette = 2) +
%   facet_wrap(~ session, ncol = 1) +
%   theme(
%     axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), 
%     axis.ticks.x =  element_line()
%   )
% 
% @
%   
%   <<>>=
%   stroop$acc %>% unique
% stroop$acc.final %>% unique
% stroop$response.final %>% unique
% 
% 
% table(stroop$acc.final, stroop$trial.type)
% table(stroop$acc.final, stroop$trial.type, stroop$session)
% 
% 
% 
% 
% 
% 
% # table(stroop$acc.final[stroop$session == "pro"], stroop$trial.type[stroop$session == "pro"])
% # table(stroop$subj, stroop$session)
% ## no responses:
% # stroop.er$acc.final %>% unique
% # stroop.er$response.final %>% unique
% 
% 
% error.rates <- stroop %>% 
%   group_by(subj, session, trial.type) %>%
%   mutate(num = n()) %>%
%   group_by(subj, session, trial.type, acc.final, num) %>%
%   summarize(freq = n()) %>%
%   mutate(prop = freq / num)
% error.rates %>%
%   ggplot(aes(session, prop, fill = trial.type)) +
%   stat_summary(position = position_dodge(width = 0.5), geom = "errorbar", fun.data = "mean_cl_boot", width = 0, size = 1) +
%   stat_summary(width = 0.5, position = position_dodge(width = 0.5), geom = "bar", fun.y = "mean") +
%   facet_wrap(~ acc.final, ncol = 5) +
%   scale_fill_brewer(type = "qual", palette = 6)
% 
% ## get subjs that have all three sessions.
% complete.bas <- subj_sum[subj_sum$session == "bas" & subj_sum$per.missing == 0, "subj"] %>% unique
% complete.pro <- subj_sum[subj_sum$session == "pro" & subj_sum$per.missing == 0, "subj"] %>% unique
% complete.rea <- subj_sum[subj_sum$session == "rea" & subj_sum$per.missing == 0, "subj"] %>% unique
% complete <- intersect(intersect(complete.bas, complete.pro), complete.rea)
% 
% library(lme4)
% stroop.er$trial.type <- as.factor(stroop.er$trial.type)
% contrasts(stroop.er$trial.type)
% 
% 
% ## is there variance to model at level of subjs?
% 
% ## overall
% 
% m.fixed <- glmer(
%   acc ~ trial.type + (1 | subj),
%   stroop.er, subset = subj %in% complete, family = "binomial"
% )
% m.rand <- glmer(
%   acc ~ trial.type + (trial.type | subj),
%   stroop.er, subset = subj %in% complete, family = "binomial"
% )
% m.rand.nocor <- glmer(
%   acc ~ trial.type + (0 + dummy(trial.type) | subj) + (1 | subj),
%   stroop.er, subset = subj %in% complete, family = "binomial"
% )
% summary(m.fixed)
% summary(m.rand)
% summary(m.rand.nocor)
% anova(m.fixed, m.rand, m.rand.nocor)
% ## keep the correlation.
% ## there is variance in trial.type across subjs.
% ## what about just for proactive (what we'll use in stroop-rsa)
% m.fixed.pro <- update(m.fixed, subset = subj %in% complete & session == "pro")
% m.rand.nocor.pro <- update(m.rand.nocor, subset = subj %in% complete & session == "pro")
% m.rand.pro <- update(m.rand, subset = subj %in% complete & session == "pro")
% summary(m.fixed.pro)
% summary(m.rand.pro)
% summary(m.rand.nocor.pro)
% anova(m.fixed.pro, m.rand.pro)
% 
% 
% ## comission
% 
% m.fixed.comm <- update(m.fixed, subset = subj %in% complete & acc.final %in% c("1", "0"))
% m.rand.nocor.comm <- update(m.rand.nocor, subset = subj %in% complete & acc.final %in% c("1", "0"))
% m.rand.comm <- update(m.rand, subset = subj %in% complete & acc.final %in% c("1", "0"))
% summary(m.fixed.comm)
% summary(m.rand.comm)
% summary(m.rand.nocor.comm)
% anova(m.fixed.comm, m.rand.comm, m.rand.nocor.comm)
% ## removing errors of omission did not reduce the variance in stroop effect across subjs to below sig levels.
% ## all's good. now for proactive alone.
% m.fixed.comm.pro <- update(m.fixed, subset = subj %in% complete & acc.final %in% c("1", "0") & session == "pro")
% m.rand.nocor.comm.pro <- update(m.rand.nocor, subset = subj %in% complete & acc.final %in% c("1", "0") & session == "pro")
% m.rand.comm.pro <- update(m.rand, subset = subj %in% complete & acc.final %in% c("1", "0") & session == "pro")
% summary(m.fixed.comm.pro)
% summary(m.rand.comm.pro)
% summary(m.rand.nocor.comm.pro)
% anova(m.fixed.comm.pro, m.rand.comm.pro, m.rand.nocor.comm.pro)
% ## removing errors of omission reduced the variance in stroop across subjects within proactive.
% 
% ## omission:
% 
% m.fixed.omm <- update(m.fixed, subset = subj %in% complete & acc.final %in% c("1", "no.response"))
% m.rand.nocor.omm <- update(m.rand.nocor, subset = subj %in% complete & acc.final %in% c("1", "no.response"))
% m.rand.omm <- update(m.rand, subset = subj %in% complete & acc.final %in% c("1", "no.response"))
% summary(m.fixed.omm)
% summary(m.rand.omm)
% summary(m.rand.nocor.omm)
% anova(m.fixed.omm, m.rand.omm, m.rand.nocor.omm)
% ## all's good. now for proactive alone.
% m.fixed.omm.pro <- update(
%   m.fixed, 
%   subset = subj %in% complete & acc.final %in% c("1", "no.response") & session == "pro"
% )
% m.rand.nocor.omm.pro <- update(
%   m.rand.nocor, 
%   subset = subj %in% complete & acc.final %in% c("1", "no.response") & session == "pro"
% )
% m.rand.omm.pro <- update(
%   m.rand, 
%   subset = subj %in% complete & acc.final %in% c("1", "no.response") & session == "pro"
% )
% summary(m.fixed.omm.pro)
% summary(m.rand.omm.pro)
% summary(m.rand.nocor.omm.pro)
% anova(m.fixed.omm.pro, m.rand.omm.pro, m.rand.nocor.omm.pro)
% ## does not seem to be much of a congruency effect in omission errors in either all sessions, or proactive.
% ## this effect also was not significantly variable across subjects.
% 
% 
% ## distribution of log odds of error:
% ## approximately normal: https://en.wikipedia.org/wiki/Odds_ratio
% coef(m.rand.comm)$subj[, "trial.typei"] %>% plot
% coef(m.rand.comm.pro)$subj[, "trial.typei"] %>% plot
% coef(m.rand.pro)$subj[, "trial.typei"] %>% plot
% 
% ## distribution of odds of error:
% coef(m.rand)$subj[, "trial.typei"] %>% exp %>% plot
% coef(m.rand.comm)$subj[, "trial.typei"] %>% exp %>% plot
% coef(m.rand.comm.pro)$subj[, "trial.typei"] %>% exp %>% plot
% coef(m.rand.pro)$subj[, "trial.typei"] %>% exp %>% plot
% 
% ## fixed effect tests of cognitive relevance of error measures ----
% 
% ## effect of session:
% 
% stroop.er$session
% m.session <- glmer(
%   error ~ trial.type + session + (1 | subj),
%   stroop.er, subset = subj %in% complete, family = "binomial"
% )
% summary(m.session)
% m.session.x <- glmer(
%   error ~ trial.type * session + (1 | subj),
%   stroop.er, subset = subj %in% complete, family = "binomial"
% )
% summary(m.session.x)
% ## baseline tends to have highest stroop effect in errors.
% 
% ## comission errors
% m.session.x.com <- glmer(
%   error ~ trial.type * session + (1 | subj),
%   stroop.er, subset = subj %in% complete & acc.final %in% c("1", "0"), family = "binomial"
% )
% summary(m.session.x.com)
% ## but not so much for only commission errors.
% 
% ## omission errors
% m.session.x.om <- glmer(
%   error ~ trial.type * session + (1 | subj),
%   stroop.er, subset = subj %in% complete & acc.final %in% c("1", "no.response"), family = "binomial"
% )
% summary(m.session.x.om)
% ## and not so much for only omission errors.
% 
% 
% 
% ## effect of previous trials:
% 
% stroop.er <- stroop.er %>%
%   arrange(subj, session, run, trial.num) %>%
%   ## NB this is techincally problematic bc there are large gaps in between blocks--- should possibly throw those 
%   ## trials out. but for a rough, first pass test...
%   group_by(subj, session, run) %>%
%   mutate(trial.type.n1 = lag(trial.type))
% stroop.er[c("trial.type", "trial.type.n1")]
% m.cse <- glmer(
%   error ~ trial.type * trial.type.n1 + (1 | subj),
%   stroop.er, subset = subj %in% complete, family = "binomial"
% )
% summary(m.cse)
% 
% ## comission?
% m.cse.com <- update(
%   m.cse, 
%   subset = subj %in% complete & acc.final %in% c("1", "0")
% )
% summary(m.cse.com)
% 
% ## omission?
% m.cse.om <- update(
%   m.cse, 
%   subset = subj %in% complete & acc.final %in% c("1", "no.response")
% )
% summary(m.cse.om)
% 
% ## proactive specifically?
% m.cse.pro <- update(
%   m.cse, 
%   subset = subj %in% complete & session == "pro"
% )
% summary(m.cse.pro)
% m.cse.com.pro <- update(
%   m.cse, 
%   subset = subj %in% complete & acc.final %in% c("1", "0") & session == "pro"
% )
% summary(m.cse.com.pro)
% 
% ## omission?
% m.cse.om.pro <- update(
%   m.cse, 
%   subset = subj %in% complete & acc.final %in% c("1", "no.response") & session == "pro"
% )
% summary(m.cse.om.pro)
% 
% 
% 
% @
%   
%   \section{Subject sample}
% 
% \bibliography{C:/local/no-url}


\end{document}